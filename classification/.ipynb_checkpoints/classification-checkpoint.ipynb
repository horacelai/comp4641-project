{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (318 of 203966) |                   | Elapsed Time: 0:00:00 ETA:   0:01:04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (203966 of 203966) |################| Elapsed Time: 4:10:49 Time:  4:10:49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>root</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_level</th>\n",
       "      <th>is_newbie</th>\n",
       "      <th>is_not_push_post</th>\n",
       "      <th>msg</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>create_time</th>\n",
       "      <th>reply_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1791346</td>\n",
       "      <td>694ee0d54ccad4523d76ca52fb8432eff03e76d9</td>\n",
       "      <td>254898</td>\n",
       "      <td>sync7272</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>其實我哋可唔可以集體起訴班狗\\n而家一句洗黑錢就收曬我地皮\\n又話8000萬捐款得2000萬...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1576861969</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1791346</td>\n",
       "      <td>1026f622b11d1b81b68fe1ce3a64da6d660d3689</td>\n",
       "      <td>254898</td>\n",
       "      <td>sync7272</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>捐錢為手足提供法律授助係合法架</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1576862115</td>\n",
       "      <td>254898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1791346</td>\n",
       "      <td>ccdc1cdc9cc1e8a5d1be888e4a2d56daf95926ad</td>\n",
       "      <td>254898</td>\n",
       "      <td>sync7272</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>沒有黑錢，只有黑警</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1576862209</td>\n",
       "      <td>254898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1791346</td>\n",
       "      <td>d095a6c5ca7b33ea5015c9a2a8c11623f77c2a2e</td>\n",
       "      <td>254898</td>\n",
       "      <td>sync7272</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>沒有黑錢，只有黑警</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1576862459</td>\n",
       "      <td>254898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>1791346</td>\n",
       "      <td>2758e161ef0ab437dc49afa8203f29ad61074839</td>\n",
       "      <td>327740</td>\n",
       "      <td>支爆女神</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>【习近平之春】知唔知 點解連續兩日 有突發事件 令連登洗版？\\nhttps://lih.kg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1576862502</td>\n",
       "      <td>254898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    root thread_id                                   post_id user_id  \\\n",
       "0   True   1791346  694ee0d54ccad4523d76ca52fb8432eff03e76d9  254898   \n",
       "1  False   1791346  1026f622b11d1b81b68fe1ce3a64da6d660d3689  254898   \n",
       "2  False   1791346  ccdc1cdc9cc1e8a5d1be888e4a2d56daf95926ad  254898   \n",
       "3  False   1791346  d095a6c5ca7b33ea5015c9a2a8c11623f77c2a2e  254898   \n",
       "4  False   1791346  2758e161ef0ab437dc49afa8203f29ad61074839  327740   \n",
       "\n",
       "  user_name user_level  is_newbie  is_not_push_post  \\\n",
       "0  sync7272         10      False             False   \n",
       "1  sync7272         10      False             False   \n",
       "2  sync7272         10      False             False   \n",
       "3  sync7272         10      False             False   \n",
       "4      支爆女神         10       True             False   \n",
       "\n",
       "                                                 msg like_count dislike_count  \\\n",
       "0  其實我哋可唔可以集體起訴班狗\\n而家一句洗黑錢就收曬我地皮\\n又話8000萬捐款得2000萬...         13             1   \n",
       "1                                    捐錢為手足提供法律授助係合法架          2             0   \n",
       "2                                          沒有黑錢，只有黑警          1             0   \n",
       "3                                          沒有黑錢，只有黑警          2             0   \n",
       "4  【习近平之春】知唔知 點解連續兩日 有突發事件 令連登洗版？\\nhttps://lih.kg...          0             0   \n",
       "\n",
       "   create_time reply_to  \n",
       "0   1576861969     None  \n",
       "1   1576862115   254898  \n",
       "2   1576862209   254898  \n",
       "3   1576862459   254898  \n",
       "4   1576862502   254898  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe = pd.read_json('../scraper/posts.json')\n",
    "\n",
    "# posts_data = dataframe.drop(['post_id', 'thread_id', 'user_name', 'msg', 'reply_to'], axis=1)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "\n",
    "path_to_json = './s3/*' \n",
    "\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "print(\"Loading JSON File\")\n",
    "\n",
    "result = []\n",
    "for f in progressbar.progressbar(file_list):\n",
    "    with open(f, \"rb\") as infile:\n",
    "        result = result + json.load(infile)\n",
    "\n",
    "posts_data = pd.DataFrame(result)\n",
    "\n",
    "# i = 0\n",
    "# for file in progressbar.progressbar(file_list):\n",
    "#     data = pd.read_json(file, lines=True)\n",
    "#     temp = temp.append(data, ignore_index = True)\n",
    "#     i+=1\n",
    "#     if i % 10000 == 0:\n",
    "#         temp.to_pickle(f\"./data_{i}.pkl\")\n",
    "    \n",
    "\n",
    "# posts_data = temp\n",
    "posts_data.to_pickle(\"data_final.pkl\")\n",
    "posts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                                          #  | 4003958 Elapsed Time: 0:09:04\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# with open('../scraper/posts.json') as f:\n",
    "#   posts = json.load(f)\n",
    "\n",
    "for index, post in progressbar.progressbar(posts_data.iterrows()):\n",
    "    # print(post)\n",
    "    if post['root'] is True:\n",
    "        G.add_node(post['user_id'])\n",
    "    else:\n",
    "        if post['is_not_push_post'] is True:\n",
    "            G.add_edge(post['user_id'], post['reply_to'], weight=0.3)\n",
    "        else:\n",
    "            G.add_edge(post['user_id'], post['reply_to'], weight=1)\n",
    "            \n",
    "hub, aut = nx.hits(G)\n",
    "\n",
    "ac = nx.eigenvector_centrality(G)\n",
    "cluster = nx.clustering(G)\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "\n",
    "nx.write_gexf(G, \"lihkg.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {int(k):v for k,v in hub.items()}\n",
    "aut = {int(k):v for k,v in aut.items()}\n",
    "\n",
    "ac = {int(k):v for k,v in ac.items()}\n",
    "cluster = {int(k):v for k,v in cluster.items()}\n",
    "betweenness = {int(k):v for k,v in betweenness.items()}\n",
    "\n",
    "posts_data['hub_score'] = posts_data['user_id'].map(hub)\n",
    "posts_data['aut_score'] = posts_data['user_id'].map(aut)\n",
    "posts_data['eigenvector_centrality'] = posts_data['user_id'].map(ac)\n",
    "posts_data['clustering'] = posts_data['user_id'].map(cluster)\n",
    "posts_data['betweenness_centrality'] = posts_data['user_id'].map(betweenness)\n",
    "\n",
    "KNOWN_TROLLS = ['41853','194398','76776','71393','30019','37596','63950','310982','282494','72457','326966','322121','322637','121095','169','213597','226126','245201','159448','149494','181778','324892','273582','118226','240250','316329','77788','219767','71341','12186','228352','75196','28435','25002','247732','322304','258598','133525','14941','27416','222907','48631','158008','289946','96230','40470','186800','30788','288523','254191','84585','85242','123870','89514','170281','103799','64699','49699','299923','71140','122676','61748','202464','288351','234658','277044','149978','6237','98232','69484','165028','328913','132380','146030','331780','266100','52773','275585','249366','41351','273474','39751','51615','326708','213952','1210','270563','241151','70493','335209','276916','232237','94819','78468','203248','100028','291185','95649','332154','83060','264328','244624','105890','5591','171664','23721','129619','326414','14965','51985','191271','253253','25771','153956','32583','72825','336250','58089','70734','124277','173333','155597','137604','276602','41818','162674','151378','295632','147880','314774','57035','88029','97104','216114','64295','28833','203010','264400','273157','238973','57405','28870','132128','199261','126609','65530','172349','173849','192248','261633','127675','34757','166655','228589','343059','144849']\n",
    "KNOWN_TROLLS = list(map(int, KNOWN_TROLLS))\n",
    "\n",
    "posts_data['troll'] = posts_data['user_id'].apply(lambda x: 0 if (x not in KNOWN_TROLLS) else 1)\n",
    "\n",
    "posts_data['root'] = posts_data['root'].apply(lambda x: 1 if x else 0)\n",
    "posts_data['is_newbie'] = posts_data['is_newbie'].apply(lambda x: 1 if x else 0)\n",
    "posts_data['is_not_push_post'] = posts_data['is_not_push_post'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "posts_data = posts_data.sample(frac=1).reset_index(drop=True)\n",
    "posts_data = posts_data.drop(['user_id'], axis=1)\n",
    "\n",
    "#posts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data[posts_data['troll'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(posts_data, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('troll')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['user_level', 'like_count', 'dislike_count', 'hub_score', 'aut_score', 'eigenvector_centrality', 'clustering', 'betweenness_centrality']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "feature_columns.append(feature_column.indicator_column(feature_column.categorical_column_with_vocabulary_list(\n",
    "      'is_newbie', [0, 1])))\n",
    "\n",
    "feature_columns.append(feature_column.indicator_column(feature_column.categorical_column_with_vocabulary_list(\n",
    "      'is_not_push_post', [0, 1])))\n",
    "\n",
    "feature_columns.append(feature_column.indicator_column(feature_column.categorical_column_with_vocabulary_list(\n",
    "      'root', [0, 1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(0.4),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(0.4),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=100, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
